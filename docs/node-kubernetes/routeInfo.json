{"template":"/Users/tainguyen/neo-one/packages/neo-one-website/src/pages/docs","sharedHashesByProp":{},"data":{"current":"/docs/node-kubernetes","title":"Kubernetes","content":{"type":"markdown","value":"\nIn this section we will cover how to deploy a NEO•ONE Node to a Kubernetes cluster.\n\nIf you are unfamiliar with Kubernetes visit their [getting started](https://kubernetes.io/docs/tutorials/kubernetes-basics/) page,\nin particular we will be implementing a [StatefulSet](https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/) locally using Kubernetes through Docker.\n\n---\n\n[[toc]]\n\n---\n\n## Requirements\n\n- [Docker](https://www.docker.com/get-started)\n  - Minimum: **_at least_** 2GB Memory, and 50GB Storage allocated\n  - Recommended: 4GB Memory, 60GB+ Storage allocated (you will need ~60GB of storage per pod)\n- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/)\n  - You can enable Kubernetes through the docker GUI; Docker >> Preferences... >> Kubernetes >> Enable Kubernetes\n\n---\n\n## Getting Started\n\nThe following deployment spec will create a StatefulSet of `n` nodes defined by `spec.replicas`. Each requests 60GB of storage and 4GB of memory. If you do not have a default storage class set (docker will automatically create one for local deployments) you will need to create one, see [storage classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) for more information.\n\nA requirement of StateFul sets is a headless service, so we'll start by creating `neo-one-service`:\n\n```yml\n#node-svc.yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: neo-one-node\n  labels:\n    app: neo-one-node\nspec:\n  ports:\n    - port: 8080\n      name: node\n  clusterIP: None\n  selector:\n    app: neo-one-node\n```\n\nfollowed by the StatefulSet itself which we'll save as `node-spec.yml`:\n\n```yml\n# node-spec.yml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: node\nspec:\n  serviceName: \"neo-one-node\"\n  replicas: 1\n  selector:\n    matchLabels:\n      app: neo-one-node\n  template:\n    metadata:\n      labels:\n        app: neo-one-node\n    spec:\n      containers:\n      - name: neo-one-node\n        image: neoonesuite/node\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          name: node\n        volumeMounts:\n        - name: node-data\n          mountPath: /root/.local/share/neo-one\n        args: [\n          \"--node.rpcURLs=http://seed6.ngd.network:10332\",\n          \"--node.rpcURLs=https://seed1.red4sec.com:10332\"\n        ]\n        resources:\n          requests:\n            memory: \"4Gi\"\n            cpu: \"1\"\n  volumeClaimTemplates:\n  - metadata:\n      name: node-data\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 60Gi\n```\n\nRunning this deployment with\n\n```bash\nkubectl apply -f node-svc.yml\nkubectl apply -f node-spec.yml\n```\n\nwill start a single pod which:\n\n- makes a persistent volume claim for 60GB of storage.\n- starts the node with this volume mounted to the default node-storage path\n- if node-data isn't present, restore from the public google-cloud backup\n- sync the node using two seeds from http://monitor.cityofzion.io/\n\nThere are two main benefits to deploying the nodes this way. If a pod needs to restart for _any_ reason it will always attempt to bind to the same persistent volume and will not start until it is scheduled on the same machine as that volume. It also makes it incredibly simple to scale the number of nodes you would like to run.\n\n## Pause/Shutdown\n\nThe simplest way to pause any pods from scheduling/running is by setting `spec.replicas: 0` in your `node-spec.yml` and running\n\n```bash\nkubectl apply -f node-spec.yml\n```\n\nTo purge the persisted volume space and shutdown the headless service you can simply delete both with:\n\n```bash\nkubectl delete svc neo-one-service\nkubectl delete statefulset node\n```\n\n## Configuration\n\n[Configuration Reference](/docs/node-configuration/)\n\nWhile the above examples shows how to configure our pods with environment variables we can also mount a [ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/) to our pods. As an example we will use a basic configuration:\n\n```bash\n#/path/to/config.json\n{\n  \"telemetry\": {\n    \"logging\": {\n      \"level\": \"debug\"\n    }\n  }\n}\n```\n\nCreating a ConfigMap is as easy as running:\n\n```bash\n#create the configMap\nkubectl create configmap example-config-map --from-file=config=/path/to/config.json\n#inspect the configMap\nkubectl describe configmap example-config-map\n```\n\nThen, we can apply the ConfigMap by modifying the node-spec above to the following:\n\n```yml\n# node-spec.yml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: node\nspec:\n  serviceName: 'neo-one-node'\n  podManagementPolicy: 'Parallel'\n  replicas: 1\n  selector:\n    matchLabels:\n      app: neo-one-node\n  template:\n    metadata:\n      labels:\n        app: neo-one-node\n    spec:\n      containers:\n        - name: neo-one-node\n          image: neoonesuite/node\n          ports:\n            - containerPort: 8080\n              name: node\n          volumeMounts:\n            - name: node-data\n              mountPath: /root/.local/share/neo-one\n            - name: config-volume\n              mountPath: /etc/neo-one/config\n              subPath: config\n          resources:\n            requests:\n              memory: '4Gi'\n              cpu: '1'\n      volumes:\n        - name: config-volume\n          configMap:\n            name: example-config-map\n            optional: true\n\n  volumeClaimTemplates:\n    - metadata:\n        name: node-data\n      spec:\n        accessModes: ['ReadWriteOnce']\n        resources:\n          requests:\n            storage: 60Gi\n```\n\n::: warning\n\nNote\n\nBecause of how [rc](https://www.npmjs.com/package/rc) searches for configuration files the key of the configMap MUST be `config` no extension.\n\n:::\n\n## Health Probes\n\nSometimes a node will go down and need to be restarted, more commonly a node gets 'stuck' while syncing the blockchain data from other nodes. Because of this it can be extremely useful to configure health probes alongside your pods.\n\nTo enable health checks on the NEO•ONE Node you must first enable the options in your configuration. Assuming you have set up configuration mounting described above the following example `config.json` will enable health checks:\n\n```json\n{\n  \"node\": {\n    \"rpcURLs\": [\n      \"http://seed1.ngd.network:10332\",\n      \"http://seed2.ngd.network:10332\",\n      \"http://seed3.ngd.network:10332\",\n      \"http://seed4.ngd.network:10332\",\n      \"http://seed5.ngd.network:10332\",\n      \"http://seed6.ngd.network:10332\",\n      \"http://seed7.ngd.network:10332\",\n      \"http://seed8.ngd.network:10332\",\n      \"http://seed9.ngd.network:10332\",\n      \"http://seed10.ngd.network:10332\",\n      \"https://seed1.cityofzion.io:443\",\n      \"https://seed2.cityofzion.io:443\",\n      \"https://seed3.cityofzion.io:443\",\n      \"https://seed4.cityofzion.io:443\",\n      \"https://seed5.cityofzion.io:443\"\n    ]\n  },\n  \"network\": {\n    \"listenTCP\": {\n      \"port\": 8081\n    }\n  },\n  \"rpc\": {\n    \"http\": {\n      \"port\": 8080\n    },\n    \"liveHealthCheck\": {\n      \"rpcURLs\": [\n        \"http://seed1.ngd.network:10332\",\n        \"http://seed2.ngd.network:10332\",\n        \"http://seed3.ngd.network:10332\",\n        \"http://seed4.ngd.network:10332\",\n        \"http://seed5.ngd.network:10332\",\n        \"http://seed6.ngd.network:10332\",\n        \"http://seed7.ngd.network:10332\",\n        \"http://seed8.ngd.network:10332\",\n        \"http://seed9.ngd.network:10332\",\n        \"http://seed10.ngd.network:10332\",\n        \"https://seed1.cityofzion.io:443\",\n        \"https://seed2.cityofzion.io:443\",\n        \"https://seed3.cityofzion.io:443\",\n        \"https://seed4.cityofzion.io:443\",\n        \"https://seed5.cityofzion.io:443\"\n      ]\n    },\n    \"readyHealthCheck\": {\n      \"rpcURLs\": [\n        \"http://seed1.ngd.network:10332\",\n        \"http://seed2.ngd.network:10332\",\n        \"http://seed3.ngd.network:10332\",\n        \"http://seed4.ngd.network:10332\",\n        \"http://seed5.ngd.network:10332\",\n        \"http://seed6.ngd.network:10332\",\n        \"http://seed7.ngd.network:10332\",\n        \"http://seed8.ngd.network:10332\",\n        \"http://seed9.ngd.network:10332\",\n        \"http://seed10.ngd.network:10332\",\n        \"https://seed1.cityofzion.io:443\",\n        \"https://seed2.cityofzion.io:443\",\n        \"https://seed3.cityofzion.io:443\",\n        \"https://seed4.cityofzion.io:443\",\n        \"https://seed5.cityofzion.io:443\"\n      ]\n    }\n  }\n}\n```\n\nBe sure to apply this new config to your ConfigMap `example-config-map` by running:\n\n```bash\nkubectl delete configmap example-config-map\nkubectl create configmap example-config-map --from-file=config=/path/to/config.json\n```\n\nAfter our config has been mapped we can add the liveness/readiness configurations to our `node-spec.yml`:\n\n```yml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: node\nspec:\n  selector:\n    matchLabels:\n      app: neo-one-node\n  serviceName: 'neo-one-node'\n  podManagementPolicy: 'Parallel'\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: neo-one-node\n    spec:\n      containers:\n        - name: neo-one-node\n          image: neoonesuite/node\n          ports:\n            - containerPort: 8080\n              name: node\n          volumeMounts:\n            - name: node-data\n              mountPath: /root/.local/share/neo-one\n            - name: config-volume\n              mountPath: /etc/neo-one/config\n              subPath: config\n          resources:\n            requests:\n              memory: '4Gi'\n              cpu: '1'\n          livenessProbe:\n            httpGet:\n              path: /live_health_check\n              port: 8080\n            initialDelaySeconds: 30\n            periodSeconds: 60\n          readinessProbe:\n            httpGet:\n              path: /ready_health_check\n              port: 8080\n            initialDelaySeconds: 30\n            periodSeconds: 15\n      volumes:\n        - name: config-volume\n          configMap:\n            name: example-config-map\n            optional: true\n\n  volumeClaimTemplates:\n    - metadata:\n        name: node-data\n      spec:\n        accessModes: ['ReadWriteOnce']\n        resources:\n          requests:\n            storage: 60Gi\n```\n\nAfter starting this StatefulSet with\n\n```bash\nkubectl apply -f node-spec.yml\n```\n\nyou should be able to inspect the state of the health checks (once they start running) with\n\n```bash\nkubectl describe pod node-0\n```\n\nsee [Kubernetes Documentation](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-readiness-probes) for more information on health probes.\n\n## Exposing Pods of a StatefulSet Locally\n\nSince we start our StatefulSet under a headless service we do not have direct access to the underlying endpoints (see [headless services](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services)). The solution to this issue is by creating _another_ service which explicitly targets the pods created by the stateful set. For our single pod example it is as simple as starting a service:\n\n```yml\n#node-0-svc.yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: node-0-svc\nspec:\n  type: LoadBalancer\n  externalTrafficPolicy: Local\n  selector:\n    statefulset.kubernetes.io/pod-name: node-0\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n```\n\nVoilà! You should now be able to access the pod locally! You can test this by running\n\n```bash\ncurl localhost:8080/live_health_check\n```\n"},"link":"packages/neo-one-website/docs/3-node/2-kubernetes.md","sidebar":[{"title":"Installation","numbered":false,"subsections":[{"title":"Getting Started","slug":"/docs/getting-started"},{"title":"Environment Setup","slug":"/docs/environment-setup"},{"title":"CLI","slug":"/docs/cli"},{"title":"Playground","slug":"/docs/playground"}]},{"title":"Main Concepts","numbered":true,"subsections":[{"title":"Hello World","slug":"/docs/hello-world"},{"title":"Blockchain Basics","slug":"/docs/blockchain-basics"},{"title":"Smart Contract Basics","slug":"/docs/smart-contract-basics"},{"title":"Standard Library","slug":"/docs/standard-library"},{"title":"Properties and Storage","slug":"/docs/properties-and-storage"},{"title":"Methods","slug":"/docs/methods"},{"title":"Events and Logs","slug":"/docs/events-and-logs"},{"title":"Calling Smart Contracts","slug":"/docs/calling-smart-contracts"},{"title":"Client APIs","slug":"/docs/client-apis"},{"title":"Smart Contract APIs","slug":"/docs/smart-contract-apis"},{"title":"Testing","slug":"/docs/testing"},{"title":"Decentralized Apps","slug":"/docs/dapps"},{"title":"Deployment","slug":"/docs/deployment"}]},{"title":"Advanced Guides","numbered":false,"subsections":[{"title":"Native Assets","slug":"/docs/native-assets"},{"title":"Forward Values","slug":"/docs/forward-values"},{"title":"User Accounts","slug":"/docs/user-accounts"},{"title":"Extended Client APIs","slug":"/docs/extended-client-apis"},{"title":"Raw Client APIs","slug":"/docs/Raw-client-apis"},{"title":"React","slug":"/docs/react"},{"title":"Angular","slug":"/docs/angular"},{"title":"Vue","slug":"/docs/vue"},{"title":"dAPI Support","slug":"/docs/dapi-support"},{"title":"Configuration Options","slug":"/docs/config-options"}]},{"title":"Node","numbered":false,"subsections":[{"title":"Local Docker Development","slug":"/docs/node-docker"},{"title":"Kubernetes","slug":"/docs/node-kubernetes"},{"title":"Docker Compose","slug":"/docs/node-compose"},{"title":"Building From Source","slug":"/docs/node-source"},{"title":"Heroku Deployment","slug":"/docs/node-heroku"},{"title":"Configuration Reference","slug":"/docs/node-configuration"}]},{"title":"Contributing","numbered":false,"subsections":[{"title":"How to Contribute","slug":"/docs/how-to-contribute"},{"title":"Codebase Overview","slug":"/docs/codebase-overview"},{"title":"Smart Contract Compiler","slug":"/docs/smart-contract-compiler"},{"title":"Code of Conduct","slug":"/docs/code-of-conduct"}]}],"next":{"slug":"/docs/node-compose","title":"Docker Compose"},"previous":{"slug":"/docs/node-docker","title":"Local Docker Development"}},"path":"docs/node-kubernetes"}
